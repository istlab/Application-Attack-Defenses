\documentclass[conference]{IEEEtran}

\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{fancyhdr}
%\usepackage{lastpage}
\usepackage{amsmath}
\usepackage{amscd}
\usepackage{color}
\usepackage{url}
%\usepackage{moreverb}
\usepackage{verbatim}
\usepackage{textcomp}
\usepackage{mathptmx}
\usepackage{dingbat}
\usepackage{pifont}
\usepackage{acronym}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{supertabular}
\usepackage{listings}
\usepackage{threeparttable}
\usepackage{pdflscape}
\usepackage{array}
\usepackage{multirow}
\usepackage{subfigure}
\usepackage{array}

\usepackage[numbers,sort]{natbib}
\lstset{columns=flexible,showstringspaces=false}

\renewcommand{\algorithmiccomment}[1]{\hfill {\tt //} #1}
\newcommand{\tick}{\ding{52}}
\newcommand{\xmark}{\ding{56}}

\makeatletter
\newcommand{\thickhline}{%
    \noalign {\ifnum 0=`}\fi \hrule height 1pt
    \futurelet \reserved@a \@xhline
}
\newcolumntype{"}{@{\hskip\tabcolsep\vrule width 1pt\hskip\tabcolsep}}
\makeatother

\date{}
\begin{document}

% \author{
% \IEEEauthorblockN{Dimitris Mitropoulos,\IEEEauthorrefmark{1}
% Panos Louridas,\IEEEauthorrefmark{2}
% Michalis Polychronakis,\IEEEauthorrefmark{1}
% and Angelos D. Keromytis\IEEEauthorrefmark{1}}
% \IEEEauthorblockA{\IEEEauthorrefmark{1}
% Network Security Lab\\
% Department of Computer Scinence\\
% Columbia University\\
% \{dimitro, mikepo, angelos\}@cs.columbia.edu\\
% \IEEEauthorrefmark{2}
% Software Engineering and Security Lab\\
% Department of Management Science and Technology\\
% Athens University of Economics and Business\\
% louridas@aueb.gr
% }}

\author{Anonymous Submission}

\title{SoK: Defending Against Web Application Attacks: From Research
  Approaches to Practical Tools}

\maketitle
\thispagestyle{plain}
\pagestyle{plain}
\begin{abstract}

  Some of the most dangerous web application attacks, such as Cross-Site
  Scripting and SQL injection, exploit vulnerabilities
  in web applications that may accept and process
  data of uncertain origin without proper validation or
  filtering, allowing the injection and execution of
  dynamic or domain-specific language code.
  These attacks have been constantly topping the lists of
  various security bulletin providers despite the numerous
  countermeasures that have been proposed over the past 15 years.

  In this paper, we systematize the existing knowledge on
  various defense mechanisms against source code injection attacks by proposing
  a model that highlights the key weaknesses that enable these attacks,
  and provides a common perspective for the different defenses.
  We then categorize and analyze the various defense mechanisms that
  have been developed, based on their accuracy, performance,
  deployment, security, and availability properties.
  We discuss the results of our analysis, with emphasis on factors
  that may hinder the widespread adoption of attack detection
  mechanisms in practice.

\end{abstract}

\begin{IEEEkeywords}
Security and Protection, Web Application Security, Code injection Attacks,
Testing, Cross-Site Scripting.
\end{IEEEkeywords}

\IEEEpeerreviewmaketitle

\section{Introduction}

Web application attacks may involve security misconfigurations, broken
authentication and session management, or other issues. Some of the
most dangerous and prevalent web application attacks, however,
exploit vulnerabilities associated with improper validation or filtering
of untrusted inputs, resulting in the injection of malicious
script or domain-specific language code.
Attacks of this type include Cross-Site Scripting ({\sc xss})
attacks~\cite{SG07}, {\sc sql} injection~\cite{RL12b}, and
Cross-Site Request Forgery ({\sc csrf}) attacks~\cite{LZRL09}, among
others.

For the past several years,
these attacks have been topping the lists of the most dangerous vulnerabilities
published by {\sc owasp}~\cite{OWASPtop10},
{\sc mitre}~\cite{MITREtop25}, and other organizations.
For instance, consider the case of {\sc owasp}'s popular Top Ten
project~\cite{OWASPtop10},
which aims to raise awareness about web application security by
identifying some of the most critical risks organizations may face.
In its three consecutive Top Ten lists (2007, 2010, 2013), different source
code-driven injection attacks dominate the top five positions.

At the same time, attackers find new ways~\cite{HNSHS12,DKH14}
to bypass defense mechanisms using a variety of techniques,
despite the numerous countermeasures that are being introduced.
As an example, already by 2006,
there were more than 20 proposed defenses
against {\sc sql} injection attacks~\cite{HVO06}.
Since then, the number has doubled, while researchers have indicated that
the number of {\sc sql} injection attacks has been steadily
increasing in recent years~\cite{SSL12}.

In this paper, we explore how different attacks associated with 
the exploitation of untrusted input validation errors can be modeled 
under a {\it common perspective}. To that end, we propose an exploitation
model which highlights that most of the steps needed to mount
different types of source code injection attacks are
common.\footnote{Note that we do not consider lower-level attacks based on
the exploitation of memory corruption vulnerabilities and the injection
of binary code---see Section~\ref{sec:attacks}.} This is validated by the fact that
there are protection mechanisms that can defend against more than one of these
types of attacks. Then, we {\it categorize} a selection of representative
protection mechanisms and finally we {\it analyze} each mechanism across the
following dimensions:
\begin{itemize}
\item {\it Accuracy:} protection mechanisms are as good
  as their detection capability; this requires low false positive and
  false negative rates.
\item {\it Availability:} whether the mechanism and its
  testbed are publicly available.
\item {\it Computational performance:} the computational
  overhead of the mechanisms at their points of deployment.
\item {\it Ease of use:} whether the protection
  mechanism is practical in terms of deployment
  and can be easily adopted by security experts.
\item {\it Security:} the robustness of the protection mechanism against
  attackers with knowledge of its internals who attempt to circumvent it.
\item {\it Detection Point:} the location where a mechanism detects an attack
  based on our exploitation model.
\end{itemize}

\noindent
All the above requirements are considered important
when building security mechanisms for the protection of
applications~\cite{A01,A00,SPWS13,nature2014}.
Based on this analysis, we identify the advantages
and disadvantages of the
various mechanisms and enumerate some of their common
characteristics. We also draw useful conclusions
about the various protection categories and see how they compare
to each other.

There is already a survey on mitigating software vulnerabilities in
general~\cite{SZ12}. The scope of that research is much broader that
ours and leaves out the majority of the approaches and mechanisms that
we consider in this work. This is mainly because that work includes
countermeasures that identify vulnerabilities using program analysis
(which takes place during the development or testing phases), while we
consider protection mechanisms that counter web attacks when they take
place. Similarly, static or dynamic analysis techniques that examine
applications to identify vulnerabilities that may lead to the attacks
that we described earlier are out of scope as well. While
countermeasures against {\sc sql} injection attacks have been
previously surveyed in a 2006 paper~\cite{HVO06}, a significant number
of new {\sc sql} injection defenses has been proposed since then, as
we mentioned earlier.

The main contributions of this paper are the following:

\begin{enumerate}
\item We provide a unified exploitation model for different types of
  web application attacks based on source code injection.
\item We categorize and analyze proposed defenses using a set of
  criteria that are important for building protection mechanisms.
\item We provide insight based on the issues that arise from our analysis. We put
  emphasis on factors that may hinder the widespread deployment of
  protection mechanisms, and the transition of tools from research to practice.
\end{enumerate}

The rest of the paper is organized as follows.
Section~\ref{sec:attacks} provides some insights on
source code injection attacks and
Section~\ref{sec:model} presents our proposed model.
Section~\ref{sec:dimensions} introduces the dimensions
across which we analyze the various defenses.
Our categorization and analysis is presented in
Section~\ref{sec:defs} and a discussion is provided in
Section~\ref{sec:discussion}. Finally,
Section~\ref{sec:lessons-learned} highlights
some lessons learned from our observations
and Section~\ref{sec:conclusion} concludes the paper.

\section{Source Code Injection Attacks\\in Web Applications}
\label{sec:attacks}

Lack of input validation is a major vulnerability behind dangerouw web
application attacks. By taking advantage of this, attackers can inject
their code into applications to perform malicious tasks. Exploits of
this kind can have different forms depending on the execution context
of the application and the location of the programming flaw that leads
to the attack.

Bratus et al.~\cite{BLSPS11} portray the issue in a more generic way:
{\it ``unexpected (and unexpectedly powerful) computational models
  inside targeted systems, which turn a part of the target into a
  so-called `weird machine' programmable by the attacker via crafted
  inputs (a.k.a. `exploits').''} In particular, {\it ``every
  application that copies untrusted input verbatim into an output
  program is vulnerable to code injection.''} Ray and
Ligatti~\cite{RL12b} have proved this claim based on formal language
theory.

\begin{figure}
\begin{center}
\leavevmode
\includegraphics[width=\columnwidth]{attack-tree-uml.pdf}
\end{center}
\caption{\label{fig:taxonomy}A taxonomy of dynamic and domain-specific
language code injection attacks against web applications.}
\end{figure}

Code injection attacks can be divided in two categories. The first
involves binary code and the second source language code. An
extensive survey on binary code injection attacks was conducted by
Lhee and Chapin~\cite{LC03}. Advances in memory corruption vulnerability
exploitation have been studied extensively~\cite{PB04, SPWS13}
and countermeasures to such attacks
have already been surveyed~\cite{YJP12}\cite[Sec. 13.8]{DKZ12}.
In this work
we do not consider binary code injection, focusing instead on defenses
that protect web applications against attacks based on the
injection of source code.

Figure~\ref{fig:taxonomy} presents a taxonomy of source
code injection attacks against web applications.
Such attacks may involve high-level language code, written in either a
{\it Domain Specific Language} ({\sc dsl}) or a {\it Dynamic Language}.
To illustrate, we briefly discuss examples from both categories.

Injection attacks that involve {\sc dsl}s constitute an important
subset of the source code injection problem, as {\sc dsl}s such as
{\sc sql} and {\sc xml} play a significant role in the development of
both web and mobile applications. For example, many applications have
interfaces through which a user enters input to interact with the
application, thereby interacting with the underlying database. This
input can become part of an {\sc sql} query and get executed on the
target database. Code injection attacks that exploit vulnerabilities
in database interfaces by taking advantage of input validation issues,
such as incorrectly passed parameters or incorrect type handling, are
called {\sc sql} injection attacks~\cite{CERT02,MS09,HVO06,SW06}.
Consider a trivial exploit that takes advantage of incorrectly
filtered quotation characters in an application that sends a temporary
password for a forgetful user via email by executing the following
query:

\bgroup \lstset{language=SQL}
\begin{small}
\begin{lstlisting}
SELECT password from userdata WHERE id = 'Alice'
\end{lstlisting}
\end{small}
\egroup

\noindent
Attackers that would input the string {\tt anything' OR 'x'='x}
thereby obtaining the administrator's password by email. Savvy
programmers can use certain {\sc api} functions, such as {\sc php}'s
{\tt mysql\_real\_escape\_string()}, to detect malformed input, or,
better, use prepared {\sc sql} statements instead of statement
templates. Unfortunately, the increasing number of {\sc sql} injection
attacks suggests that programmers are not always that careful. Using
similar techniques, malicious users can mount other exploits based on
{\sc dsl}s such as {\sc xp}ath~\cite{SW06,CDL07,MKS09}, {\sc
  xml}~\cite{MSM13} and {\sc json}~\cite{SMS13}. The effects can be
wide-ranging. A malicious user can view sensitive information, destroy
or modify protected data, or even crash the entire application.

{\sc html} is another {\sc dsl} that can be used for malicious
purposes when an application does not properly handle user-supplied
data. Based on this vulnerability attackers can supply valid {\sc
  html}, typically via a parameter value, and inject their own content
into the page. {\sc html} injection is mainly associated with {\sc
  xss} attacks~\cite{BJM08,LZRL09}. However, it can also be used as a
vehicle for cross-site request forgery ({\sc csrf}) attacks in the
following manner: Consider a vulnerable bulletin board system where
{\tt img} tags are not checked. A malicious user could embed a {\sc
  csrf} request within an {\tt img} tag in the following manner:

\bgroup \lstset{language=HTML}
\begin{small}
\begin{lstlisting}
<img src='http://www.vulnerable.com/
  admin.php?edituserwithID=13&addgroup=admin'/>
\end{lstlisting}
\end{small}
\egroup

\noindent
When the page with this injected code is accessed by an administrator,
the attacker (with {\sc id} 13) will gain administrative privileges,
while the administrator will have no immediate indication that there
has been an attack.

A recent class of code injection attacks involve dynamic languages
such as JavaScript and {\sc php}~\cite{SFVM09,SMS13}.
A simple example of a {\sc php} injection attack is an input string
that is fed into an {\tt eval()} function call, e.g.:

\bgroup \lstset{language=PHP}
\begin{small}
\begin{lstlisting}
$variable = $_GET['var']; 
$input = $_GET['value'];
eval('$variable = ' . $input . ';')
\end{lstlisting}
\end{small}
\egroup

\noindent
The user may pass into the {\tt value} parameter code that will
be executed on the server. Hence, if an attacker provides as
input the following string: {\tt 10 ; system("touch foo");}
then a file will be created on the server---it is easy
to imagine more detrimental scenarios.

JavaScript injection attacks make up a large subset of dynamic
language code injection attacks. Such attacks are enabled when
a web application accepts
and redisplays data of uncertain origin without
proper validation and filtering. Based on this flaw, an attacker
can manage to inject a script in the JavaScript engine of a browser
and alter its execution flow~\cite{ELX07}.
JavaScript injection attacks are considered a critical issue
in web application security mainly because it is associated with
major vulnerabilities such as {\sc xss} attacks~\cite{SG07} and
Cross-Channel Scripting ({\sc xcs}) attacks~\cite{W10,BBB09}.
For a typical {\sc xss} example that involves JavaScript injection
consider a web page that prints the value
of a query parameter ({\tt query}) from the
page's {\sc url} as part of the page's content
without escaping the value. Attackers
can take advantage of this and inject an {\tt iframe} tag
into the page to steal a user's cookie and
send it via an image request to a web site
under their control (malicious.com).
This could be achieved by including the following
link to the malicious web site (or sending it via phishing
email) and inducing the user to click on it:

\lstset{language=VBScript, basicstyle=\footnotesize\ttfamily,}
\begin{lstlisting}
http://example.com/vulnerable.html?query=
<iframe src="javascript:document.body.innerHTML=+
'<img src=\"http://malicious.com/?c=
'+encodeURIComponent(document.cookie)+'\">'">
</iframe>
\end{lstlisting}

\noindent
Note that in many cases {\sc xss} attacks
involve the injection of both {\sc html} and JavaScript code.

\section{Exploitation Model}
\label{sec:model}

We provide a step-by-step exploitation model to understand the process
of carrying out source code injection attacks against web
applications. Figure~\ref{fig:attacks} presents the required steps for
different classes of injection attacks. Links between steps are
labeled with the different attacks that use that path, while the
numbers next to attack labels denote the sequence of steps for a
particular attack. Most steps are common in all attacks. This is
because they are based on similar attack vectors as we mentioned in
the previous section. Large `X' marks on transitions indicate the
points where defenses detect or prevent attacks.

An attacker can initiate an injection attack through two different main routes.
One way is to use the browser of a victim as an attack
vehicle, through which the code will be injected in the application.
For example, the attacker could craft a malicious script into a {\sc
  url} and then trick a user to click on it through social
engineering, e.g., by sending a phishing email (transition {\sc p-xss} 1.1, {\sc
  n-xss} 1.1, {\sc csrf} 1). Alternatively, the attacker may be able
to inject directly the malicious code on the server through an HTTP request
({\sc dsl} 1, {\sc p-xss} 1, {\sc n-xss} 1). This would happen
in a web application that accepts and processes user input
without appropriate validation. An attacker could upload data
containing a specially crafted script
to steal the cookies of the visiting users ({\sc p-xss} 1, {\sc n-xss} 1)
or embed malicious
{\sc sql} code to retrieve the password entries from a database ({\sc dsl} 1).
%Recall the examples we provided in Section~\ref{sec:attacks}.
Note that {\sc xss} attacks can start from both routes.

Once the injected code reaches the vulnerable application, it becomes
a part of a value represented by a program variable. The target of the
attack determines the route from now on. In an {\sc sql} injection
attack the injected code becomes part of a query that
finally reaches the database where it is executed.

{\sc xss} attacks fall into two categories, {\it non-persistent} and
{\it persistent}. Non-persistent {\sc xss} attacks take place when the
data provided by a user is processed on-the-fly
by server-side application logic and end up without proper sanitization
into a dynamically generated response ({\sc p-xss} 7, {\sc n-xss} 2)
that is eventually rendered by the user's browser.
Thus, in a non-persistent {\sc xss} attack, the injected code
is not saved on the server and immediately becomes part of the
content that is sent back to a web user.
In persistent {\sc xss} attacks, on the other hand,
malicious code is permanently stored on the server ({\sc p-xss} 5).
The injected code residing at server-side then re-enters the
application's execution flow and becomes part of the content that is
actually sent back to the user as part of a future response.

In the case of a {\sc csrf} attack, consider a
user who visits a page, where attackers have managed
to inject {\sc html} code that contains a hidden
request in the way we described in Section~\ref{sec:attacks}.
If an authenticated user visits this web page, then the
malicious request will reach the application ({\sc csrf} 2) and
the functionality dictated by the malicious response will
be performed.

\begin{figure*}
\begin{center}
\leavevmode
\includegraphics[scale=0.61]{attacks-steps-newer.pdf}
\end{center}
\caption{\label{fig:attacks}Attack model for dynamic and domain specific
language code injection in web applications.
Transitions are labeled with the different attacks that use that path,
while the numbers next to attack labels denote the 
sequence of steps for a particular attack.
Points where different types of defenses
detect or prevent attacks are marked with an `X' symbol:
{\sc ub} (at the browser):~\cite{KJKV09,LV09,TNH07,NSS06,APKLM10,ML10,YCIS07,PSC09,VDDPJ11,OWVS08,DDHPJ10,VFJKKV07,SLMS14,BV08},
{\sc s}t{\sc b} (en route from the server to the browser):~\cite{RDWDE07,JKK06a,GC09,JB07,NLC07,WPLKK09,JEP08,PS11},
{\sc dbal} (at the database abstraction layer):~\cite{BWS05,SW06,HCF05,XBS06,PB05,PMP11,MS09,HO05,SMS13},
i{\sc db} (at the database):~\cite{BK04,LLW02,VMV05}.}
\end{figure*}

\section{Analysis Dimensions}
\label{sec:dimensions}

Research on application attack defense mechanisms has a dual
character. First, a defense mechanism may be important, and therefore
publishable, because it shows that an attack can be detected in a
reliable manner. Secondly, a defense mechanism may be important not
just as a research contribution, but as a practical tool, if it can be
put to use by administrators and users to shield their applications
and services against the detected attacks.

The detection of attacks in a reliable manner can be analyzed using
criteria common with other research fields:
\begin{itemize}
\item Statistical measures that show how reliable the detection really
  is.
\item Research practices that promote replication of the findings.
\end{itemize}

Whether a reliable attack defense mechanism has value in a practical
setting rests on a different set of criteria:
\begin{itemize}
\item What are the overheads imposed by the mechanism?
\item How easy is it to deploy and use the mechanism?
\item How robust is it against ways to circumvent it?
\item At which point of the computation flow does it detect an attack?
\end{itemize}

The first two criteria are \emph{Accuracy} and
\emph{Availability}. The other four criteria are \emph{Computational
  Performance}, \emph{Ease of Use}, \emph{Security}, and
\emph{Point of Detection}.

\subsection{Accuracy}
\label{ssec:diagnostic-performance}

Web application attack defense mechanisms
must demonstrate the presence of an attack. This, however, does not
make an application attack defense mechanism immediately useful. A
detection mechanism must be reliable. The accuracy of a detection
mechanism is gauged with the following
metrics~\cite{TDR2013,GFDLS06,A00}:
\begin{itemize}
\item {\bf Sensitivity}, the probability that an attack will be
  caught.
\item {\bf Specificity}, the probability that a normal interaction
  will test negative.
\item {\bf Positive Predictive Value} ({\sc ppv}), the probability that a
  reported attack is a real attack. It is the conditional probability
  that an event is an attack if the detection mechanism flags it as
  such. 
\item {\bf Negative Predictive Value} ({\sc npv}), the probability that if
  nothing is reported no attack has taken place. It is the conditional
  probability that an event is not an attack given that the detection
  mechanism flags it as normal.
\end{itemize}

We will focus on sensitivity and specificity; we will come back to
\textsc{ppv} and \textsc{npv} in Section~\ref{sec:lessons-learned}.
Sensitivity and specifity are defined using the following~\cite{linn2004}:
\begin{itemize}
\item True Positive (\textsc{tp}), an attack that raises an alarm.
\item True Negative (\textsc{tn}), an event that is not an attack and that does
  not raise an alarm.
\item False Positive (\textsc{fp}), an event that although it is not an attack
  raises an alarm.
\item False Negative (\textsc{fn}), an event that although is as attack does
  not raise an alarm.
\end{itemize}

\noindent
With these we can calculate:

\begin{equation}
  \textsc{se} = \textrm{Sensitivity} = \frac{\textsc{tp}}{\textsc{tp}
    + \textsc{fn}}
\label{eq:sensitivity}
\end{equation}

\begin{equation}
  \textsc{sp} = \textrm{Specificity} = \frac{\textsc{tn}}{\textsc{fp}
    + \textsc{tn}}
\label{eq:specificity}
\end{equation}

\noindent
Sensitivity and specificity can be calculated based on test
data alone. To calculate the sensitivity, we run the test on a
controlled environment where we allow only attack events to reach the
system. The ratio of reported attacks over all attacks will give us
the sensitivity. Similarly, to calculate the specificity we can run
the test on a controlled environment where we allow only innocuous
events to reach the system. The ratio of non-reported events over all
events will give us the specificity. 

\subsection{Availability}

To ensure replicability of research results the source code
implementing a detection mechanism should be available to researchers.
Ideally the code should be available under an open source license, so
that it is easy to modify and improve an approach; even when this is
not possible, for whatever reason, it is important to make sure that
all computer code and test data is available noting any restrictions
on accessibility. Availability of computer code has been recognized as
an important issue outside the computer science field: the journal
\emph{Nature}, in a recent editorial, adopted a publication policy stressing
access to code and data~\cite{nature2014}. At a time and date that the
merits of open access to code are discussed in non computer science
journals~\cite{easterbrook2014}, we should expect computer scientists
to lead the way.
 
\subsection{Computational Performance}

All detection mechanisms extract a cost for their use, as they
introduce some amount of extra computation on an existing application.
The extra computation depends on the exact defense mechanism. For
example, it may be some form of run-time checking, or some form of
obfuscation. Also depending on the mechanism, the cost may be incurred
on different places: it may take place on a server, affecting service
performance, or it may take place on the client, or both. The
usefulness of a detection mechanism depends therefore on the
computational cost it requires and on where it extracts it, as
different overheads may be acceptable server-side than client-side.

What kind of numbers is reported matters as well. Reporting absolute
measurements gives little information on the real overhead, unless
separate measurements are given for the system under study with and
without the proposed mechanism. Percentage measurements are normally
better. 

Performance evaluation rests on strong foundations~\cite{jain1991} and
is a vibrant field as new technologies emerge~\cite{gregg2014}.
Although it may not be necessary to conduct a comprehensive
performance evaluation analysis for a defense mechanism, the more
performance evaluation is provided for a mechanism the more valuable
it becomes as a practical approach.

\subsection{Ease of Use}
\label{sec:deployment}

The value of a detection mechanism as a practical tool depends on how
it is to deploy it in a production setting. That is independent of the
value of a detection mechanism as a research finding. Devising a
mechanism to detect a hitherto undetectable class of attacks may be an
excellent research contribution that merits publication; it may also
be heavily cited and open the road to other, practical
implementations in the future. 

Ease of use depends on the deployment protocol required for the
mechanism. The detection mechanism may be deployed server-side or
client-side, or both. Deployments on just the server or the client are
easier to handle than deployments on both of them. The mechanism may
be an add-on, or plugin, on existing software, client or server, or it
may be tightly integrated with existing software, requiring rebuilding
from source code.

No matter where it gets installed, the means of installation influences
ease of use. A detection mechanism that comes with ready to install
packages will trump a detection mechanism that only exists in the form
of source code. 

\subsection{Security}

Defenders and attackers are often caught in a cat-and-mouse game,
where countermeasures are bypassed by canny attacks, which are caught
by more sophisticated countermeasures, yet again bypassed by cannier
attacks, and so on. We use security to refer to the ability of a
detection mechanism to resist being circumvented. 

A mechanism that has not been bypassed is not eternally secure, as it
is possible that a bypass method will be discovered in the future. We
examine the various approaches on the knowledge we have to this date,
that is, whether there are known ways to bypass the detection
mechanism today.

\subsection{Point of Detection}

Detection mechanisms vary on the location where they detect an attack.
There are four different points where an attack can be caught, as seen
in Figure~\ref{fig:attacks}:
\begin{itemize}
\item At the user's browser (Point {\sc ub}).
\item En route from the server to the user's browser---in most cases
within a proxy (Point {\sc s}t{\sc b}).
\item At the database abstraction layer, before it reaches the server's database
  (Point {\sc dbal}).
\item After the malicious code reaches the server's database
  (Point i{\sc db}).
\end{itemize}

\section{Defenses}
\label{sec:defs}

\begin{figure*} [ht]
\begin{center}
\leavevmode
\includegraphics[scale=0.65]{defenses.pdf}
\end{center}
\caption{\label{fig:defenses}The basic categories of countermeasures
against web application attacks based on source code
injection. For each approach we provide the references
that present corresponding mechanisms:
{\it Policy Enforcement}:~\cite{NSS06,JKK06a,KKVJ06,KJKV09,TNH07,RDWDE07,YCIS07,OWVS08,PSC09,ML10,DDHPJ10,PS11,VDDPJ11,BV08},
{\sc isr}:~\cite{BK04,JB07,GC09,APKLM10},
{\it Parse-Tree Validation}:~\cite{BWS05,SW06},
{\it Taint Tracking}:~\cite{HCF05,PB05,XBS06,NLC07,VFJKKV07,PMP11,SLMS14},
{\it Training}:~\cite{LLW02,HO05,HO06,HO05b,VMV05,JEP08,WPLKK09,MS09,MKS09,MKLS11},
{\it Hybrid}:~\cite{BV08,LV09,SMS13}.}
\end{figure*}

We categorize and describe the various mechanisms developed to detect
the attacks described in Section~\ref{sec:attacks} and~\ref{sec:model},
on the dimensions we proposed apart from availability, which we treat
separately in the discussion in Section (\ref{sec:discussion}).
All mechanisms have been proposed in publications cited more than
20 times. We also include some recent mechanisms that have been
presented in top security conferences, even though they have been
cited less than twenty times, so that recent research is not penalized.

Figure~\ref{fig:defenses} presents a taxonomy of the
defenses that protect web applications against
source code injection attacks based on.
We can identify three approaches:
{\it etiological}, {\it symptomatic}, and {\it hybrid}.
The etiological category involves mechanisms designed to
stop attacks based on their causes and origins~\cite{JL75,L81}. 
The symptomatic category incorporates a variety of schemes that
inspect the behavior of applications and detect attacks based on
their undesirable symptoms~\cite{D76,A00}.
Hybrid mechanisms borrow characteristics from both
categories. Table~\ref{tab:comp} groups the various subcategories and
for every mechanism provides the following:

\begin{enumerate}
\item The number of times the corresponding publication has been cited.
\item Accuracy and computational overhead measurements.
\item The attacks that the mechanism detects.
\end{enumerate}

Recall that the point where attacks are detected is given in the
caption of Figure~\ref{fig:attacks}.

\subsection{Etiological}
\label{sec:prot}

There exist three basic etiological approaches used to protect web
applications from source code injection attacks.

\subsubsection{Parse-Tree Validation}
\label{sec:tree}

The key idea behind parse-tree validation is to compare the tree
representation of the abstract syntactic structure of the source code
that is about to be executed with the one that was originally
intended. If the trees diverge, the application is probably under
attack.

In the case of {\sc dsl} injection attacks, mechanisms check 
the query before the inclusion of user input with the one
resulting after the inclusion of user input.
Both mechanisms that implement this approach,
{\sc sqlg}uard~\cite{BWS05} and
{\sc sql}check~\cite{SW06} are pretty similar
and detect the attack before a query reaches the
database (Point {\sc dbal}).
Contrary to {\sc sqlg}uard and other
mechanisms, {\sc sql}check has been
extensively tested in terms of accuracy
as can be seen in Table~\ref{tab:comp}.
A disadvantage of those mechanisms is that the
application must be modified in every code fragment
that sends an {\sc sql} query to the database for execution.

Parse-tree validation seems to be an efficient approach
to detect {\sc dsl} code injection attack,
especially in the case of the {\sc sql}check
implementation. This however does not apply in
the case of the mechanisms that borrow elements from
this approach and examine the syntax trees
of scripts to detect JavaScript-driven {\sc xss} attacks,
as we will see in Subsection~\ref{sec:hybrid}.

\subsubsection{Policy Enforcement}
\label{sec:policy}

This approach is used to detect {\sc xss} and {\sc csrf} attacks. When
using a framework that implements policy enforcement, developers must
define specific security policies on the server side. Policies can be
expressed through JavaScript extensions, pattern matching, or syntax
specific settings. Then the policies are enforced either in the user's
browser at runtime or in a server-side proxy that intercepts server
responses.

{\it Noxes}~\cite{KKVJ06,KJKV09} is the only
framework that partially allows the web
user to specify policies to prevent {\sc xss} attacks.
The key idea behind Noxes is to parse
the {\sc html} that reaches the browser to
find static {\sc url} references. Then, based on
a set of policies Noxes allows or blocks
the request (Point {\sc ub}). Such policies
can be also provided by the server (i.e., ``never
follow a link that leads to the malicious.com
web site''). The main issue with Noxes is that
it does not deal with JavaScript-driven {\sc xss}
attacks, which currently make for the most popular
{\sc xss} attack vector.

A number of the frameworks define policies based on information and
features provided by the Document Object Model ({\sc dom}) of a web
page. Specifically, developers must place all legitimate scripts
inside {\sc html} elements like {\tt div}. The web browser (Point {\sc
  ub}), parses the {\sc dom} tree and executes scripts only when they
are contained in such elements. All other scripts are treated
according to the policies defined on the server. Frameworks that
support this functionality are {\sc beep}~\cite{TNH07} and {\sc
  dsi}~\cite{NSS06}. The main problem with these mechanisms is that
they do not examine the script's location inside the web document.
Based on this fact, attackers can perform mimicry attacks~\cite{WS02}.
Specifically, they can execute legitimate scripts, but not as intended
by the original design of the developers. This is extensively
described by {\it Athanasopoulos et al.}~\cite{APKLM10}, who also describe
another recent variation of JavaScript injection attacks, known as
return-to-JavaScript attacks that can be used to bypass the above
mechanisms.

Another policy enforcement approach introduces
policies directly either in {\sc html} or JavaScript code
to confine their behavior. {\it BrowserShield}~\cite{RDWDE07}
acts as a proxy on the server side (Point {\sc s}t{\sc b}) to
parse the {\sc html} of server responses and identify
scripts. Then, it rewrites them into safe equivalents
and protects the web user from exploits
that are based on reported browser vulnerabilities.
{\it ConScript}~\cite{ML10}, {\it CoreScript}~\cite{YCIS07}
and the framework by {\it Phung et al.}~\cite{PSC09}
extend JavaScript with new primitive functions that
provide safe methods to protect potentially vulnerable
JavaScript functions. In both cases, policy enforcement takes
place on the JavaScript engine of the browser (Point {\sc ub}).
In this way, {\sc xxs} attacks that take advantage
functions like {\tt write} and {\tt eval}, which are
used to assemble innocuous-looking parts into harmful
strings,\footnote{The infamous Sammy worm that
infected MySpace in 2005~\cite{SP07,ELX07}
utilized the {\tt eval} function to assemble a
script with malicious behavior.} would fail.

An issue regarding the above frameworks
involves features like script inclusion
and {\tt iframe} tags. Even though they allow developers
decide if they will disable them or not,
this is unpractical because such features are quite popular.
If developers choose to use them, these frameworks cannot
define policies that restrict the behavior of third-party
scripts introduced by such features. Thus, they would
be vulnerable in an attack that utilizes {\tt iframes}
in the way we described in Section~\ref{sec:attacks}.
\noindent {\it WebJail}~\cite{VDDPJ11}, and {\sc soma}~\cite{OWVS08}
are two frameworks that can actually detect such an attack (Point {\sc
  ub}). To achieve this, {\sc soma} requires site administrators to
specify legitimate, external domains for sending or receiving
information in order to approve interactions between them and the
protected web site. As a result, {\sc soma} can also detect {\sc csrf}
attacks. WebJail contains the functionality of third-party scripts by
introducing a web component integrator that restricts the access that
these scripts may have to either the data or the functionality of
other components.

Policy enforcement mechanisms that detect {\sc csrf}
attacks are usually implemented in the form of a
server-side proxy (Point {\sc s}t{\sc b})
interposed on the client-server communication and modifying
it. {\it NoForge}~\cite{JKK06a}
parses the {\sc html} server responses
and adds a token to every {\sc url} referring to this
server. Then it associates the token with the cookie
representing the session {\sc id} for the application.
When a request is received, the mechanism checks
if the request contains the token related
to the session {\sc id}. A disadvantage of NoForge
is that {\sc html} that is dynamically created within
the browser will not include the token.
Thus, sites that create their {\sc html} on the client
will remain vulnerable. In addition, it does not
support cross-origin requests.
{\it j{\sc csrf}}~\cite{PS11} has a similar
functionality and meanwhile addresses the above problems.
Finally, {\it CsFire}~\cite{DDHPJ10}
examines cross-domain interactions, to design a
cross-domain policy on the client's side (Point {\sc ub}).
The policy is based on the concept of a relaxed
same-origin policy that allows communication between
sub-domains of the same registered domain.

Most of the aforementioned frameworks involve
several deployment issues since they
require significant source modifications by the
developers on the server-side in order to
introduce the policies.

\subsubsection{Instruction Set Randomization (ISR)}

{\sc isr} is a method that has been applied to counter different kinds
of application attacks~\cite{K09b,KKP03}. The main idea behind it is
to separate code from data by randomizing the legitimate code's
execution environment. In this way, the malicious code injected by the
attackers who do not know the randomization algorithm will not get
executed.

{\it {\sc sql}rand}~\cite{BK04} implements {\sc isr} to detect {\sc
  sql} injection attacks. It allows programmers to create {\sc sql}
statements using randomized instructions instead of standard keywords.
The modified queries are reconstructed at runtime using the same key
used for randomization, which is inaccessible to the malicious user.
{\sc sql}rand is one of the few mechanisms that detect {\sc sql}
injection attacks on the database level (Point i{\sc db}).

In the case of {\sc xss} attacks that are based on JavaScript or {\sc
  html}, consider a {\sc xor} function that encodes all source of a
web page on the server-side and then, on the client-side, the web
browser decodes the source by applying the same function again either
in a proxy at the server-side (Point {\sc s}t{\sc b}) or in the browser
(Point {\sc ub}). Variations of this approach include {\it
  Noncespaces}~\cite{GC09} and {\it x{\sc js}}~\cite{APKLM10}, which
randomize the instruction set of {\sc html} and JavaScript
respectively. Contrary to x{\sc js}, in Noncespaces administrators
must set specific policies in a manner similar to a firewall
configuration language. {\it {\sc sm}ask}~\cite{JB07} is another
framework that was inspired by {\sc isr}. To detect {\sc xss}
attacks it searches for {\sc html} and
JavaScript keywords within the application's legitimate code. This is
done before the processing of an {\sc http} request. When it finds one
it adds a token to it. This results to a ``code mask.'' Then, before
sending the resulting {\sc html} data to the web user, the framework
searches the data for illegal code by using the same keywords (Point
{\sc s}t{\sc b}). Since all legitimate code has been ``masked,'' the injected
code can be identified. The pre- and post-processing of the code,
though, may add a significant overhead to the application.
Unfortunately, the authors of {\sc sm}ask did not provide measurements
regarding the computational performance of the tool (see
Table~\ref{tab:comp}).

{\sc isr} is a deterministic approach that can be applied to detect
different attacks in an efficient manner. However, {\it Sovarel et
al.}~\cite{SEP05} have investigated thoroughly the effectiveness of
{\sc isr} and showed that a malicious user may be able to circumvent
it by determining the randomization key. Their results indicate
that doing {\sc isr} in a way that provides a certain degree of
security against a motivated attacker is more difficult than
previously thought. Furthermore, developers who wish to use such
mechanisms must follow good coding practices and never show a
randomized code statement in any case (i.e., an exception error)
because in this way they will reveal the encoding key.

Even though the above implementations impose a low computational
overhead, they impose a deployment infrastructure overhead. In particular, in
{\sc sql}rand requires the integration of a proxy within the database
server, while Noncespaces and x{\sc js} require modifications on both
the server and the client.

\begin{table*}
    \caption{Comparison summary of mechanisms developed to counter application attacks based on source code injection.}
    \label{tab:comp}
\centering
    \begin{threeparttable}
    \begin{small}
\scalebox{0.93}{
    \begin{tabular}{l|c|c|cc|c}
    \thickhline
    \bf{Approach}
  & \bf{Mechanism}
  & \bf{\# of Citations}
  & \multicolumn{2}{|c|}{Requirements\tnote{1}}
  & \bf{Attack}\tnote{4} \\
  &&& \bf{TP,TN,FP,FN}\tnote{2}
  & \bf{Computational Overhead}\tnote{3} & \\
    \thickhline
  \multirow{2}{*}{Parse-Tree Validation}
  &   {\it {\sc sqlg}uard}~\cite{BWS05} & 243 & ({\sc na},{\sc na},{\sc na},{\sc na})\_{\bf ?} & 3\% ({\sc s}) & {\sc sql}i \\
  &   {\it {\sc sqlc}heck}~\cite{SW06} & 402 & (36848,7648,0,0)\_r & 3ms per query ({\sc s}) & {\sc sql}i \\
  \hline
  \multirow{12}{*}{Policy Enforcement}
  &   {\it {\sc dsi}}~\cite{NSS06} & 135 & (5268,{\sc nq},{\sc nq},85)\_r & 1.85\% ({\sc c}) & {\sc xss} \\ 
  &   {\it NoForge}~\cite{JKK06a} & 35 & (7,{\sc nq},{\sc nq},0)\_r & {\sc na} ({\sc s}) & {\sc csrf} \\
  &   {\it Noxes}~\cite{KKVJ06,KJKV09} & 268,40 & (3,{\sc na},{\sc na},0)\_r & {\sc na} ({\sc c}) & {\sc xss} \\
  % &   {\it {\sc met}}~\cite{ELX07} & 58 & ({\sc na},{\sc na},{\sc na},{\sc na})\_{\bf ?} & {\sc na} & {\sc xss} \\ 
  &   {\it {\sc beep}}~\cite{TNH07} & 282 & (61,{\sc na},{\sc na},0)\_r & 14.4\% ({\sc c}) & {\sc xss} \\
  &   {\it BrowserShield}~\cite{RDWDE07} & 219 & (19,{\sc nq},0,0,)\_r & 8\% ({\sc s}) & {\sc xss} \\ 
  &   {\it CoreScript}~\cite{YCIS07} & 181 & ({\sc nq},{\sc nq},{\sc nq},{\sc na})\_s & {\sc nq} ({\sc c}) & {\sc xss} \\
  &   {\it {\sc soma}}~\cite{OWVS08} & 46 & (5,{\sc na},{\sc na},0)\_s & 5.58\% ({\sc c}) & {\sc xss}, {\sc csrf} \\
  % &   {\it Barth et al.}~\cite{BJM08} & 271 & & {\bf ?} & {\sc csrf} \\
  &   {\it Phung et al.}~\cite{PSC09} & 75 & (37,{\sc na},{\sc na},4)\_r & 5.37\% ({\sc c}) & {\sc xss} \\
  &   {\it ConScript}~\cite{ML10} & 122 & ({\sc na},{\sc na},{\sc na},{\sc na})\_{\bf ?} & 7\% ({\sc c}) & {\sc xss} \\
  &   {\it CsFire}~\cite{DDHPJ10} & 35 & (419582,1141807,0,3)\_r\tnote{5} & {\sc na} ({\sc c}) & {\sc csrf} \\
  &   {\it j{\sc csrf}}~\cite{PS11} & 2 & (2,{\sc na},{\sc na},0)\_r & 2ms ({\sc s}) & {\sc csrf} \\
  &   {\it WebJail}~\cite{VDDPJ11} & 25 & (2,{\sc na},{\sc na},1)\_{\bf ?} & $\sim$6.89ms ({\sc c}) & {\sc xss} \\
  % &   {\it TreeHouse}~\cite{IW12} & 18 & ({\sc na},{\sc na},{\sc na},{\sc na})\_{\bf ?} & 757–-1218ms & {\sc xss} \\
  % &   {\it {\sc js}and}~\cite{AVBPDP12} & 22 & ({\sc na},{\sc na},{\sc na},{\sc na})\_{\bf ?} & up to 31.2\% & {\sc xss}\\
  % \hline
  % \hline 
  % \multirow{3}{*}{Securing Mashups}
  % &   {\it {\sc om}ash}~\cite{CHC08} & 58 & ({\sc na},{\sc na},{\sc na},{\sc na})\_{\bf ?} & {\sc nq} & {\sc csrf} \\
  % &   {\it Mashup{\sc os}}~\cite{WFHJ07} & 149 & ({\sc na},{\sc na},{\sc na},{\sc na})\_{\bf ?} & 1--59\% & {\sc xss} \\
  \hline
  \multirow{4}{*}{{\sc isr}}
  &   {\it {\sc sql}rand}~\cite{BK04} & 286 & (3,{\sc na},{\sc na},0)\_a & $\le$6.5{\it ms} ({\sc s}) & {\sc sql}i \\
  &   {\it {\sc sm}ask}~\cite{JB07} & 27 & (5,{\sc nq},{\sc nq},{\sc nq})\_r  & {\sc na} ({\sc s}) & {\sc sql}i, {\sc xss} \\
  &   {\it Noncespaces}~\cite{GC09} & 109 & (6,{\sc na},{\sc na},0)\_r &  2\% ({\sc s}) & {\sc xss} \\ 
  &   {\it x{\sc js}}~\cite{APKLM10} & 18 & (1380,{\sc na},{\sc na},1)\_r & 1.6--40{\it ms} ({\sc c}) & {\sc xss} \\
  \thickhline
  \thickhline
  \multirow{7}{*}{Taint Tracking}
  &   {\it Haldar et al.}~\cite{HCF05} & 177 & (2,{\sc na},{\sc na},0)\_s & {\sc nq} ({\sc s}) & {\sc sql}i, {\sc xss} \\ 
  &   {\sc csse}~\cite{PB05} & 312 & (7,{\sc nq},{\sc nq},{\sc nq})\_r & 2--10\% ({\sc s}) & {\sc sql}i, {\sc xp}athi, {\sc xss} \\
  % &   {\it SecuriFly}~\cite{MLL05} & 31 & \xmark,\tick & 9--125\% & {\sc sql} injection, {\sc xss} \\ 
  &   {\it Xu et al.}~\cite{XBS06} & 297 & (9,{\sc nq},0,{\sc nq})\_r & average 76\% ({\sc s}) & {\sc sql}i, {\sc xss} \\ 
  &   {\it {\sc wasc}}~\cite{NLC07} & 31 & ({\sc nq},{\sc nq},{\sc nq},{\sc nq})\_r & up to 30\% ({\sc s}) & {\sc sql}i, {\sc xss} \\
  &   {\it Vogt et al.}~\cite{VFJKKV07} & 322 & ({\sc nq},{\sc nq},{\sc nq},{\sc na})\_r & {\sc nq} ({\sc c}) & {\sc xss} \\
  &   {\it {\sc php} Aspis}~\cite{PMP11} & 12 & (12,{\sc nq},{\sc nq},2)\_r & 2.2$\times$ ({\sc s}) & {\sc sql}i and {\sc php}i, {\sc xss} \\
  &   {\it Stock et al.}~\cite{SLMS14} & 0 & (1169,{\sc na},{\sc na},0)\_r & 7--17\% ({\sc c}) & {\sc dom}-based {\sc xss} \\
  \hline 
  \multirow{6}{*}{Training}
  &   {\it {\sc didafit}}~\cite{LLW02} & 85 & ({\sc na},{\sc na},{\sc na},{\sc na})\_{\bf ?} & {\sc na} ({\sc s}) & {\sc sql}i \\
  &   {\it {\sc amnesia}}~\cite{HO05,HO06,HO05b} & 118,66,376 & (1470,{\sc nq},0,0)\_a & {\sc nq} ({\sc s}) & {\sc sql}i \\ 
  &   {\it libAnomaly}~\cite{VMV05} & 226 & (9,15987,60,0)\_r & 0.20--1{\it ms} per query ({\sc s}) & {\sc sql}i \\
  &   {\it {\sc xssds}}~\cite{JEP08} & 64 & ({\sc nq},{\sc nq},{\sc nq},0)\_r & {\sc nq} ({\sc s}) & {\sc xss} \\
  &   {\it {\sc swap}}~\cite{WPLKK09} & 52 & ({\sc nq},{\sc nq},{\sc nq},{\sc nq})\_r & up to 261 {\it ms} ({\sc s}) & {\sc xss} \\ 
  &   {\it {\sc sd}river}~\cite{MS09,MKS09,MKLS11} & 21,8,5 & (241,{\sc nq},0,0)\_a & 39\% ({\sc s}) & {\sc sql}i and {\sc xp}athi \\
  % &   {\it Laranjeiro et al.}~\cite{LVM09,ALVM09,LVM10} & 9,40,1 & \xmark,\xmark  & \xmark & {\sc sql} and {\sc xp}ath injection \\
  \thickhline
  \thickhline
  \multirow{3}{*}{Hybrid}
  &   {\it {\sc xss-guard}}~\cite{BV08} & 97 & (8,{\sc nq},{\sc nq},{\sc nq})\_r & 5--24\% ({\sc c}) & {\sc xss} \\
  &   {\it Blueprint}~\cite{LV09} & 110 & (94,{\sc na},{\sc na},0)\_r & 13.6\% ({\sc c}) & {\sc xss} \\
  &   {\it Diglossia}~\cite{SMS13} & 4 & (25,{\sc nq},{\sc nq},{\sc nq})\_r & 13\% ({\sc s}) & {\sc sql}i and {\sc json}i \\
  \thickhline
    \end{tabular}}
    \begin{tablenotes}
  \begin{footnotesize}
        \item[1] {\sc na} (Not Available) means that a requirement is not mentioned in the paper.
  {\sc nq} (Not Quantified) indicates that a requirement is mentioned in the\newline publication
  but it is not quantified.
      \item[2] Quadruples contain numbers given for True Positives
        ({\sc TP}), True Negatives ({\sc tn}), False Positives ({\sc
          fp}) and False Negatives ({\sc fn}). For every quadruple\newline there is a corresponding suffix that indicates whether the testbed was
  based on: a) real-world applications known to be vulnerable ({\it r}), b) synthetic\newline benchmarks ({\it s}), c) both ({\it a}).
  If no testing has taken place we add a question mark ({\bf ?}).
    \item[3] Regarding the computational overhead we show if it is applied to the server ({\sc s}) or the client ({\sc c}). 
    \item[4] {\it i} stands for injection.
    \item[5] The numbers of this quadruple involve requests.
  \end{footnotesize}
    \end{tablenotes}
    \end{small}
    \end{threeparttable}
\end{table*}

\subsection{Symptomatic}

Symptomatic approaches adopt one of two tacks. They either track
untrusted input and ban certain operations on it, or they first learn what
code to trust and then approve for execution code that they recognize
as safe.

\subsubsection{Taint Tracking}
\label{sec:taint}

A taint tracking scheme marks untrusted (``tainted'') data (for
instance, a variable set by a field in a web form) and traces its flow
through the program. If the variable is used in an expression that
sets another variable, that variable is also marked as untrusted and
so on. If any of these variables is used in a potentially risky
command (for example, send the data to a vulnerable ``sink,'' such as
a database, a file, the network) the scheme may act accordingly.

Taint tracking is provided as a feature in some programming languages,
such as Perl and Ruby. By enabling the feature, Perl would refuse to
run code vulnerable to an {\sc sql} injection attack (consider a
tainted variable being used in a query) and would exit with an error
message.

There are different implementations of this approach
in terms of how the tainted data is marked and observed,
and how attacks are detected.
For example, {\it Haldar et al.}~\cite{HCF05} have implemented
their scheme for the Java Virtual Machine ({\sc jvm})
where they instrument various classes. When a
tainted string is used as an argument to a sink method
an exception is raised (Point {\sc dbal}).

It is possible to apply further checks when it is established that
tainted data have reached a sink (Point {\sc dbal}). {\it Xu et
  al.}~\cite{XBS06} track taint information at the level of bytes in
memory. To distinguish between legitimate and malicious uses of
untrusted data that reach a sink, they search the data for suspicious
symbols by using regular expressions. {\sc csse}~\cite{PB05}
associates tainted data with specific metadata. Such metadata include
the origins of tainted data, its propagation within the application,
and others. When tainted data reaches a sink, {\sc csse} performs
syntactic checks based on the aforementioned metadata (Point {\sc dbal}).
{\it {\sc php} Aspis}~\cite{PMP11} works in a similar way. To obtain
metadata, it takes advantage of the  {\sc php} array data structure.
Finally, {\sc wasc}~\cite{NLC07} analyzes {\sc html} responses to check
if there are tainted data that contain scripts (Point {\sc s}t{\sc b}).

A recent study~\cite{NBR14} showed that there are ways to circumvent
the majority of the above schemes. Furthermore, most of them are not
easy to deploy since the majority of input vectors, string operations,
and output vectors of the application must be instrumented.

{\it Vogt et al.}~\cite{VFJKKV07} have developed a tainting scheme
that follows a different approach. Contrary to the aforementioned
schemes that operate on the server side, they track sensitive
information on the client side (Point {\sc ub}). This is a form of
positive taint tracking, where taint data is considered to be
legitimate. Their scheme detects JavaScript-driven {\sc xss} attacks
by ensuring that a script can send sensitive user data only to the
site from which it came from.
{\it Stock et al.}~\cite{SLMS14} propose a scheme that also operates
in the browser (Point {\sc ub}). The scheme focuses on the detection of
{\sc dom}-based {\sc xss} attacks. In such an attack, the malicious
payload is executed as a result of the modification of the {\sc dom}
environment so that the client-side code runs in an unanticipated
manner even if the {\sc http} response that triggers the injection
seems legitimate. The scheme is different from the previous one
because it marks and observes data that are considered harmful.
Specifically, it employs a taint-enhanced JavaScript engine that
tracks the flow of attacker-controlled data. To detect potential
attacks the scheme utilizes {\sc html} and JavaScript parsers that can
identify the generation of code coming from tainted data.

\subsubsection{Training}
\label{sec:train}

Training techniques are based on the ideas of Denning's original
intrusion detection framework~\cite{Den87}. In particular, a training
mechanism registers all valid legitimate code statements during a
training phase (mostly in the form of signatures). This can be done in
various ways according to the implementation. Then, only those
statements will be recognized and approved for execution during
production.

Training methods that detect {\sc dsl}-driven injection attacks
generate and store valid code statements (i.e., {\sc sql} or {\sc
  xp}ath queries) in various forms, and detect attacks as outliers
from the set of valid code statements. An early approach, {\it {\sc
    didafit}}~\cite{LLW02} detects {\sc sql} injection (Point
{\sc sdb}) attacks by recording all database transactions
stripped from user input. Subsequent
refinements by Valeur et al.~\cite{VMV05} tagged each transaction with
the corresponding application as an extension of their anomaly
detection framework called {\it libAnomaly}. {\it {\sc
    sd}river}~\cite{MS09,MKS09,MKLS11} is a signature-based mechanism
that prevents {\sc sql} and {\sc xp}ath injection attacks. The
signatures generated during a training phase are based on features
that can depend either on the code statement or on its execution
environment (i.e., the stack trace). Then, at runtime, the mechanism
checks all statements for compliance and can block code statements
containing injected elements (Point {\sc dbal}). {\sc
  amnesia}~\cite{HO05,HO06,HO05b} is a tool that also detects {\sc
  sql} injection attacks (Point {\sc dbal}) by associating a query model
with the location of every {\sc sql} statement within the application.
Then, at runtime, it monitors the application's execution to detect
when {\sc sql} statements diverge from the expected model.

Various countermeasures against {\sc xss} attacks follow a similar
pattern. {\it {\sc swap}}~\cite{WPLKK09} creates a unique identifier
({\it script {\sc id}}) for every legitimate script on the server.
Then, a JavaScript detection component placed in a web proxy (Point
{\sc s}t{\sc b}) searches for injected scripts with no corresponding {\sc id}
in the server's responses. If no injected scripts are found, the proxy
forwards the request to the client. This mechanism is relatively
inflexible since it does not support dynamic scripts. In addition it
imposes a significant overhead (see Table~\ref{tab:comp}). The authors
of {\it {\sc xssds}}~\cite{JEP08} have implemented a similar mechanism
that also supports dynamic and external scripts. Specifically, during the
training phase they build a list of all benign scripts. In the case of
external scripts they keep a white list of all the valid domain names
that contain scripts used by the application.

Defenses based on training include some mechanisms that can be easily
circumvented. For example, {\sc didafit} and {\it libAnomaly} do not
tag transactions with their corresponding call sites. This can lead
easily to false negatives. For instance, recall the application
mentioned in Section~\ref{sec:attacks}, which will send the password
for a forgetful user via email by executing the following query:

\bgroup \lstset{language=SQL}
\begin{small}
\begin{lstlisting}
SELECT password from userdata WHERE id = 'Alice'
\end{lstlisting}
\end{small}
\egroup

\noindent
This same application could allow users to lock their terminal,
but allow the unlocking either with the user's password or with
the administrator password (the 4.3 {\sc bsd} {\em lock} command
behaved in this peculiar way).
The corresponding query to verify the password on the locked
workstation would be as follows:

\bgroup
\lstset{language=SQL}
\begin{small}
\begin{lstlisting}
SELECT password from userdata WHERE id = 'Alice' OR
id = 'admin'
\end{lstlisting}
\end{small}
\egroup

\noindent
Even if the application's administrators have chosen to use
either {\sc didafit} or {\it libAnomaly} as protection,
an attacker could bypass them and obtain the
administrator's password via email,
by entering on the form the standard string:
{\tt nosuchuser' OR id = 'admin}. The infected query matches the
signature of the second one above and is therefore accepted.
The problem lies with the call location of the query, not the query alone.

Mechanisms that create signatures that involve elements not only
associated with the code statements (i.e., {\sc amnesia} and {\sc
  sd}river) could detect such attacks. Specifically, {\sc sd}river
associates a complete stack trace with the root of an {\sc sql}
statement, thus it can correlate queries with their call sites and
detect attacks like the aforementioned one. Furthermore, training
tools that detect {\sc xss} attacks based on JavaScript injection
would fail to detect mimicry attacks where legitimate scripts can be
executed by attackers, but not in the way intended by the the
developers~\cite{APKLM10}.

In general, the detection accuracy of training approaches is heavily
based on the coverage that is achieved during the training phase. If
the coverage is insufficient false alarms are very likely. In
addition, when a code statement is altered, a new training phase is
necessary.

Most of the training approaches are relatively easy to deploy.
{\sc dsl} injection attack countermeasures
can be retrofitted to a system typically by changing
some configuration files (i.e., {\sc sd}river). This does not apply
to {\sc amnesia} though, since significant source code
modifications are required for every query that exists
in the application. Finally, {\sc swap} and {\sc xssds}
are implemented within a proxy on the server-side.

\subsection{Hybrid}
\label{sec:hybrid}

This category includes mechanisms that borrow
characteristics from both of the above categories.
Two of them focus on the detection of {\sc xss}
attacks and one of them focuses on {\sc dsl} code
injection attacks.

{\sc xss-guard}~\cite{BV08} is a training scheme that employs
parse-tree validation. During training, the scheme maps legitimate
scripts to {\sc http} responses. During production {\sc xss-guard}
retrieves for every script included in a response its parsed tree and
checks if it is one of those previously mapped to this response. Apart
from the comparison of the parsed trees, {\sc xss-guard} checks also
for an exact match of lexical entities. To achieve this, the scheme
utilizes that data structures of Firefox's JavaScript engine (Point
{\sc ub}). However, string literals are not compared literally, which
can lead to false negatives. For instance, consider a banner rotator
that every time it runs it creates a value that depends on the
current date and the length of the array that contains the references
of the various images to be displayed. Then, based on this value it
shows a specific image to a user. In a vulnerable web site that allows
users to post data and contains this banner rotator, a malicious user
could create and store a script that has the same code structure, with
the same JavaScript keywords contained in the rotator script.
%% PL You mean like the rotator?
In this script the attacker could also include references to tiny
images hosted on a web server that is maintained by him in order to
retrieve the {\sc ip} addresses of the users that visit the vulnerable
site.

{\it Blueprint}~\cite{LV09} is a policy enforcement framework that
uses parsed trees to detect {\sc xss} attacks. To guarantee
that untrusted content is not executed, Blueprint generates on the
server-side a parsed tree from untrusted {\sc html} to ensure that it
does not contain any dynamic content. Then the parsed tree is
transfered to the document generator of the browser (Point {\sc ub}),
where untrusted browser parsing behavior is ruled out.
Blueprint is an efficient countermeasure but it imposes an
overhead due to its extensive parsing (see Table~\ref{tab:comp}).

{\it Diglossia}~\cite{SMS13} combines positive taint tracking together
with parse-tree validation. Diglossia was based on the theory of Ray
and Ligatti~\cite{RL12b} (which we saw in Section~\ref{sec:attacks})
to detect {\sc dsl} code injection attacks. In addition, it is
actually the first, and so far the only framework that detects {\sc
  json} injection attacks. When an application computes an output
string (query), Diglossia computes a ``shadow'' of this string.
Specifically, it maps all characters introduced by the application to
a shadow character set. This set does not contain any characters
coming from the tainted input. Then, the scheme creates the tree of
the query that is about to be executed and compares it with the parsed
tree of the ``shadow'' (Point {\sc dbal}). If the trees are not
equal the application is probably under attack. Note that Diglossia
can be bypassed in the same way as other taint tracking
approaches~\cite{NBR14}.

\begin{table*}
\caption{Availability of the corresponding mechanisms.}
    \label{tab:avail}
\centering
    \begin{threeparttable}
    \begin{small}
\scalebox{0.93}{
    \begin{tabular}{l|c|ccc}
    \thickhline
    \bf{Approach}
  & \bf{Mechanism}
  & \multicolumn{3}{|c|}{Availability\tnote{1}} \\
  && \bf{Source Code}
  & \bf{Executable}
  & \bf{Testbed} \\
    \thickhline
  \multirow{2}{*}{Parse-Tree Validation}
  &   {\it {\sc sqlc}heck}~\cite{SW06} & {\sc na} & {\sc na} & {\sc na} \\
  &   {\it {\sc sqlg}uard}~\cite{BWS05} & {\bf AO} & {\bf AO} & {\sc na} \\
  \hline
  \multirow{12}{*}{Policy Enforcement}
  &   {\it {\sc dsi}}~\cite{NSS06} & {\sc na} & {\sc na} & {\sc na} \\
  &   {\it NoForge}~\cite{JKK06a} & {\bf AO} & {\bf AO} & {\sc na} \\
  &   {\it Noxes}~\cite{KKVJ06,KJKV09}  & {\sc na} & {\sc na} & {\sc na} \\
  % &   {\it {\sc met}}~\cite{ELX07} & {\sc na} & {\sc na} & {\sc na} \\
  &   {\it {\sc beep}}~\cite{TNH07} & \tick & \tick & \tick \\
  &   {\it BrowserShield}~\cite{RDWDE07} & {\sc na} & {\sc na} & {\sc na} \\
  &   {\it CoreScript}~\cite{YCIS07} & {\bf AO} & {\sc na} & {\sc na} \\
  &   {\it {\sc soma}}~\cite{OWVS08} & {\sc na} & {\sc na} & {\sc na} \\
  % &   {\it Barth et al.}~\cite{BJM08} & {\sc na} & {\sc na} & {\sc na} \\
  &   {\it Phung et al.}~\cite{PSC09} & \tick & \tick & \tick \\
  &   {\it ConScript}~\cite{ML10} & {\bf AO} & {\sc na} & {\sc na} \\
  &   {\it CsFire}~\cite{DDHPJ10} & \tick & {\sc na} & {\sc na} \\
  &   {\it j{\sc csrf}}~\cite{PS11} & {\sc na} & {\sc na} & {\sc na} \\
  &   {\it WebJail}~\cite{VDDPJ11} & {\sc na} & {\sc na} & {\sc na} \\
    % &   {\it {\sc js}and}~\cite{AVBPDP12} & {\sc na} & {\sc na} & {\sc na} \\
  % \hline
  % \hline 
  % \multirow{3}{*}{Securing Mashups}
  % &   {\it {\sc om}ash}~\cite{CHC08} & {\sc na} & {\sc na} & {\sc na} \\
  % &   {\it Mashup{\sc os}}~\cite{WFHJ07} & {\sc na} & {\sc na} & {\sc na} \\
  % &   {\it TreeHouse}~\cite{IW12} & {\sc na} & {\sc na} & {\sc na} \\
  \hline
  \multirow{4}{*}{{\sc isr}}
  &   {\it {\sc sql}rand}~\cite{BK04} & {\sc na} & {\sc na} & {\sc na} \\
  &   {\it {\sc sm}ask}~\cite{JB07} & {\sc na} & {\sc na} & {\sc na} \\
  &   {\it Noncespaces}~\cite{GC09} & {\sc na} & {\sc na} & {\sc na} \\
  &   {\it x{\sc js}}~\cite{APKLM10} & {\sc na} & {\sc na} & {\sc na} \\
  \thickhline
  \thickhline
  \multirow{7}{*}{Taint Tracking}
  &   {\it Haldar et al.}~\cite{HCF05}  & {\sc na} & {\sc na} & {\sc na} \\
  &   {\sc csse}~\cite{PB05} & {\sc na} & {\sc na} & {\sc na} \\
  &   {\it Xu et al.}~\cite{XBS06}  & {\sc na} & {\sc na} & {\sc na} \\
  &   {\it {\sc wasc}}~\cite{NLC07} & {\sc na} & {\sc na} & {\sc na} \\
  &   {\it Vogt et al.}~\cite{VFJKKV07}  & {\sc na} & {\sc na} & {\sc na} \\
  &   {\it {\sc php} Aspis}~\cite{PMP11} & {\bf AO} & {\sc na} & {\bf AO} \\
  &   {\it Stock et al.}~\cite{SLMS14} & {\sc na} & {\sc na} & {\sc na} \\
  \hline
  \multirow{6}{*}{Training}
  &   {\it {\sc didafit}}~\cite{LLW02} & {\sc na} & {\sc na} & {\sc na} \\
  &   {\it {\sc amnesia}}~\cite{HO05,HO06,HO05b} & {\sc na} & {\bf AO} & {\sc na} \\
  &   {\it libAnomaly}~\cite{VMV05} & {\bf ?} & {\bf ?} & {\bf ?} \\
  &   {\it {\sc xssds}}~\cite{JEP08} & {\sc na} & {\sc na} & {\sc na} \\
  &   {\it {\sc swap}}~\cite{WPLKK09} & {\sc na} & {\sc na} & {\sc na} \\
  &   {\it {\sc sd}river}~\cite{MS09,MKS09,MKLS11} & \tick & \tick & {\sc na} \\
  \thickhline
  \thickhline
  \multirow{3}{*}{Hybrid}
  &   {\it {\sc xss-guard}}~\cite{BV08} & {\sc na} & {\sc na} & {\sc na} \\
  &   {\it Blueprint}~\cite{LV09} & {\bf ?} & {\bf ?} & {\bf ?} \\
  &   {\it Diglossia}~\cite{SMS13} & {\sc na} & {\sc na} & {\sc na} \\
  \thickhline
    \end{tabular}}
    \begin{tablenotes}
  \begin{footnotesize}
       \item[1] The check mark (\tick) indicates that the publication
       includes a homepage where the reader can 
       download the corresponding software. {\bf AO} (Available On-line) suggests
       that the software is available on-line but the
       address was not mentioned in the paper, which probably indicates that
       the authors made it available after the publication. The question 
       mark ({\bf ?}) indicates that the a homepage for the software
       was included
       in the publication but now it is not available.
  \end{footnotesize}
    \end{tablenotes}
    \end{small}
    \end{threeparttable}
\end{table*}

\section{Observations}
\label{sec:discussion}

We group our key observations on the~34 publications along the
dimensions we identified in Section~\ref{sec:dimensions}.

\subsection{Accuracy}

\begin{figure}
\begin{center}
\leavevmode
\includegraphics[scale=0.47]{defect-percentages.png}
\end{center}
\caption{\label{fig:defect_sources}Pie chart showing the sources
that the used to find vulnerabilities to test
their mechanisms. We also provide which source was used
in which publication:
{\sc cve}~\cite{XBS06,NLC07,PMP11,BK04,BV08,JB07,SMS13,WPLKK09,JKK06a,PS11},
{\it Bugtraq}~\cite{PB05,KKVJ06,JEP08},
{\sc xss}ed.com~\cite{NSS06,APKLM10},
ha.ckers.org~\cite{TNH07,PSC09,LV09},
Microsoft~\cite{RDWDE07}.}
\end{figure}

Table~\ref{tab:comp} indicates that the authors of 3 out of 34 (8.8\%)
publications provided complete quadruples of {\sc tp}, {\sc tn}, 
{\sc fp}, and {\sc fn}. On the other hand, we
see that 4 out of 34 (11.7\%) were not tested at all. In these cases,
all the four elements of the quadruples are not available ({\sc na}).
An interesting observation is that in 10 out of the 34 (29.41\%)
publications, the authors performed only attacks during their tests
and did not test their mechanisms to see if they produce any false
alarms. The corresponding quadruples contain {\sc tp} and {\sc fn}
results but {\sc tn} and {\sc fp} results are not available ({\sc
  na}). This stresses the fact that authors may be more interested in
making sure that their mechanisms can detect known attacks rather than
seeing how they respond under normal conditions. In some occasions,
where the number of {\sc tp} results are not quantified, authors did
not mention how many or which attacks they performed. Moreover in some
cases we observe that even if the authors report the existence of
possible false positive and negative results, they have not quantified
them ({\sc nq}).

Even when some numbers are given, they may not be adequate.
Sensitivity and specificity are statistical measures, and as such
should be interpreted with suitable confidence intervals. There are
various methods to calculate confidence intervals, even dispensing
with the need to have large samples in order to be able to use the
central limit theorem~\cite{agresti1998,brown2001}. However, sample sizes in the single
digits are not enough to produce good intervals.

Regarding the subjects of tests, we see that 27 out of 34 mechanisms
were tested on real-world applications known to be vulnerable. The
vulnerabilities associated with these applications are enlisted in the
following providers: the Common Vulnerabilities and Exposures ({\sc
  cve}) database~\cite{cve}, the {\it Bugtraq} security
mailing list~\cite{bugtraq}, the {\sc xss}ed.com
security bulletin provider~\cite{xssed}, Microsoft's security
bulletin~\cite{microsoftBulletin} and the
ha.ckers.org security bulletin provider~\cite{hackers}.
Figure~\ref{fig:defect_sources} presents how many, in percentage,
and which, in the figure's caption, authors referred
to which source. In numerous occassions authors performed tests based
on the same applications. For example, 12 mechanisms were tested on a
vulnerable version of {\sc phpbb}.\footnote{https://www.phpbb.com/}

There are also authors that took a different approach.
For instance, during their initial tests,
Stock et al.~\cite{SLMS14} managed to bypass
the browser-based {\sc xss} filters of the
73\% of 1,602 real-world {\sc dom}-based {\sc xss} vulnerabilities.
These vulnerabilities were actually found during
their previous research~\cite{LSJ13}.
Finally, in the publications that
{\sc amnesia} and {\sc sd}river were presented,
authors managed to break existing application suites
and then test the accuracy of their tools on them.

% Comment 1: Regarding the frameworks that were
% not tested in terms of diagnostic performance
% at all, we see that they are mostly coming from
% the etiological category.

% Comment 2: Note though that implementations
% of etiological mechanisms can actually
% be bypassed.

% Comment 3: The above are some extra findings that
% indicate that there is no standard way to
% test such mechanisms. Perhaps, forming a
% standard way to test such tools in terms
% of diagnostic and computational performance
% would be one of the reasons of adopting them.

\subsection{Availability}

Table~\ref{tab:avail} presents our findings regarding the availability
of each mechanism in terms of source code and corresponding
executables. We also examined the availability of the testbeds
mentioned in the paper. We see that only 6 out of 34 (17.6\%) of the
pubications provided a homepage for their mechanism and 2 from these 6
homepages are currently not available. In 5 cases authors made either
the source code or their executables available after their paper got
published. Regarding the availability of test materials, we see that
only 3 out of 34 (8.8\%) publications have currently their testbeds
available on-line.

\subsection{Computational Performance}

Table~\ref{tab:comp} shows for every mechanism its corresponding
overhead. In addition, it indicates if the overhead is applied to the
server-side ({\sc s}), or the client-side ({\sc c}). In almost half of
the approaches, 16 out of 34 (47.05\%), the overhead is provided as a
percentage, while in other cases, 7 our of 34 (20.5\%), the authors
provide the time (in terms of ms) that their mechanisms add to the
normal execution time of the protecting entity. Note that in some
cases, the times of normal executions are not provided,
so it is not possible to move from absolute
time measurements to percentage overheads. In 10 cases (29.4\%), the
overhead was either not available ({\sc na}) or not quantified ({\sc
  nq}). In the case of {\sc php aspis}, its authors indicate that with
their mechanism, the execution time is doubled.

\subsection{Ease of Use}
\label{sec:deploy2}

Mechanisms in different categories face different deployment issues.

Consider the majority of mechanisms coming from the policy enforcement
subcategory. In most cases developers should modify multiple
components to use each mechanism. Specifically, mechanisms like
BrowserShield and {\sc beep} require modifications both on the server
and the client side. Thus, it would be difficult for them to be
adopted by both browser vendors and application developers. On the
other hand, there are cases where the policies introduced in the
server are enforced on the client-side, via a library embedded in the
server's response (i.e., Blueprint). Such an approach is convenient
since no modifications are needed on the client.

Extensive modifications in the application's source
code can also be a reason that can make a mechanism
difficult to use. {\sc sql}rand, {\sc amnesia},
mechanisms coming from the parse-tree validation
subcategory and mechanisms coming from the taint
tracking subcategory are such examples.
In the first three cases, programmers should modify every
code fragment that involves the execution of a query.
In the latter case they should also change
all the code fragments that involve user-input handling. 

Finally, even though many of the mechanisms that involve
training are easy to deploy, they have a distinct disadvantage.
When the application is altered, mechanisms like {\sc sd}river
and {\sc xss-guard} require a new training phase is necessary.
However, with the increased adoption of automated testing
and continuous integration frameworks, this phase could be
easily repeated.

\subsection{Security}

While describing the various mechanisms in Section~\ref{sec:defs}, we
observed that some can be bypassed by attackers that know how they
work. Still, some mechanisms are designed in such a way that they
can be extended and become immune to the attacks that they are
currently vulnerable to. For example, policy enforcement frameworks
that are based on JavaScript or {\sc html} rewriting can be extended
to detect attacks that leverage {\tt iframe} tags. This does not apply
to all mechanisms though. In particular, training mechanisms like {\sc
  didafit} and {\sc xss-guard} must be redesigned to detect the
attacks we described in the related sections.

Taking the attackers' point of view, mimicry attacks can affect a
range of different mechanisms: they can be used to bypass mechanisms
in the hybrid category, as well as the policy enforcement and training
subcategories.

\subsection{Point of Detection}

Given the fact that the final steps of
{\sc dsl} injection and {\sc csrf} attacks take place
on the server-side, all the mechanisms that detect
such attacks are placed in some point within the server.
From the mechanisms that detect {\sc dsl} code injection
attacks, 3 out of 13 (23\%) do so at the level
of the {\sc rdbms}---Point i{\sc db}.
The other 11 wrap {\sc api} calls related to output
vectors---Point {\sc e}i{\sc q}.

In frameworks that deal with {\sc xss} attacks we
see that attack detection may take place either on the server-side or
on the client-side. In the first case, a proxy is placed on the server
to examine the server's responses before they reach the user's web
browser. In the second case a modified browser or a library that is
securely downloaded from the server checks the responses for potential
attacks. We find that the tendency so far is to create frameworks that
perform detection on the client side: in particular, 13 out of 23
frameworks (56,5\%) detect {\sc xss} attacks on the
client's side. Note that the majority of the mechanisms that detect
such attacks on the server-side can also detect {\sc dsl} code
injection attacks (6 out of 10).

\section{Recommendations and Lessons Learned}
\label{sec:lessons-learned}

Our observations lead to some lessons and recommendations that
developers of new mechanisms may find helpful. In particular, they
call for improving accuracy tests and code availability, while
aiming to reduce computational and deployment overheads.

% @dimitro: move this to the analysis Section:
% This scheme is an efficient scheme that focuses
% on a specific category of {\sc xss} attacks and
% takes into consideration many attack vectors
% that many of the previous schemes do not.
% Also: 
% As the author of {\sc csse} state~\cite{PB05}, the prevention
% of {\sc xss} attack requires a more complex analysis.

\subsection{Improving Accuracy Tests}

One of our key findings indicate that many detect mechanisms are
tested in a poor manner. In many cases researchers tend not to provide
the false positive and negative results that their tools may produce.
Discussing about the existence of such results and not quantifying
them also blurs the picture.

A reasonable argument would be that many mechanisms (especially the
ones coming from the etiological category), do not need to be
established purely through testing since they provide systematic
arguments as to why their design is secure against attacks. In order
for this to hold, however, an implementation of a defense mechanism
may be entirely correct in terms of its specification, which may not
happen in practice. Moreover, even a mechanism that detects an attack
based on its root cause, instead of observer behavior, may still be
circumvented so that attacks will pass undetected.

When we introduced specificity and sensitivity in
Section~\ref{ssec:diagnostic-performance} we deferred discussion of
\textsc{ppv} and \textsc{npv} to this point. These two relate to the
performance of a detection mechanism in an actual production setting,
instead of a testbed. If an attack is detected in a production
environment, how much should we be worried? The answer is provided by
\textsc{ppv}. If no attack is detected in a production environment,
how relaxed should we be that no attack has indeed taken place? The
answer is provided by \textsc{npv}. We can calculate \textsc{ppv} and
\textsc{npv} with the following equations:

\begin{equation}
\textsc{ppv} = \frac{\textsc{tp}}{\textsc{tp} + \textsc{fp}}
\end{equation}

\begin{equation}
\textsc{npv} = \frac{\textsc{tn}}{\textsc{fn} + \textsc{tn}}
\end{equation}

\noindent
Equations~\ref{eq:ppv-se-sp} and~\ref{eq:npv-se-sp} use true and false
positives and negatives, like equations~\ref{eq:sensitivity}
and~\ref{eq:specificity}. However, \textsc{tp}, \textsc{tn},
\textsc{fp}, and \textsc{fn} are not the same in the two cases:
whereas sensitivity and specificity are measured on a test
environment, \textsc{ppv} and \textsc{npv} are measured on the real,
target environment. In fact, if \textsc{pr} is the probability of
attacks in the real world, the prevalence, then we
have~\cite{linn2004,altman1994}:

\begin{equation}
\textsc{ppv} = \frac{\textsc{se}\times \textsc{pr}}{
\textsc{se}\times \textsc{pr} + (1 - \textsc{sp})\times (1 -
\textsc{pr})}
\label{eq:ppv-se-sp}
\end{equation}

\begin{equation}
\textsc{npv} = \frac{\textsc{sp}\times (1 - \textsc{pr})}{
(1 - \textsc{se})\times \textsc{pr} + \textsc{sp}\times (1 -
\textsc{pr})}
\label{eq:npv-se-sp}
\end{equation}

\noindent
The prevalence is the prior probability that an event is an attack,
based on our understanding of the volume and frequency of attacks; the
\textsc{ppv} and \textsc{npv} are the revised estimates of the
probability based on the results of the detection
mechanism~\cite{altman1994}. The lower the prevalence of an attack,
the more confident we can be that a negative test result indicates
that no attack has taken place and the less sure we can be that a
positive test result indicates a real attack. 

Of course it is not easy, and it may not even be possible, to know how
prevalent an attack is. Also, attacks on a system may depend on
factors such as its visibility and popularity, so that the same piece
of software may be subject to varying attack regimes depending on
where it is actually deployed. With this in mind it may be unfair to
ask of researchers to provide \textsc{ppv} and \textsc{npv} values for
their mechanisms---in fact, we see that nobody does.

That does not mean that deriving \textsc{ppv} and \textsc{npv} is
altogether impossible. One could deploy a system, armoured with an
attack detection mechanism, on a honeypot to study what happens over
a length of time. This could give an indication of the performance of
the mechanism out in the wild. Alternatively, one could deploy a
target, unarmoured system, on a honeypot to study the prevalence of
the class of attacks to be detected. Studying the prevalence of
classes of attacks could be an interesting area of study on its own,
which could feed directly on the evaluation of attack detection
mechanisms as practical tools.

Apart from demonstrating the value of a mechanism, good accuracy tests
may be beneficial per se, by leading to more well-designed and robust
mechanisms. Mechanisms that can be circumvented were not extensively
tested in terms of accuracy. For instance, {\sc didafit} was not
tested at all and {\sc xss-guard}'s testing involved 6 known attacks.
This also applies to some frameworks coming from the taint tracking
subcategory. It is possible that more tests during development would
have led the authors to wider improvements in design~\cite{Van14}.

The accuracy of a tool may be related to the scope of the attacks it
aims to detect. A more limited scope may allow the development of more
accurate tools. In this vein, the system by Stock et al~\cite{SLMS14}
is the only one that targets exclusively {\sc dom}-based {\sc xss}
attacks and detects them in an accurate manner as seen in
Table~\ref{tab:comp}. It is also extensively tested in real-world
attacks.

\subsection{On Code Availability}

Apart from testing practices, another area that merits improvement is
the availability of source code and testbeds. We are not aware of
specific reasons why authors of attack detection mechanisms seem to be
wary of releasing on-line their code and tests. Our finding may reflect
the status in the current point in time, when authors are urged to
publish their code and tests, and may start, or may have already
started doing so, but this does not show yet in the research we
examined, which goes several years back. We also saw instances where
material was published, but does not seem to be available any more.
That points to the importance of curation of research materials. That
is a problem arising in all scientific fields. It is not enough to
publish the underlying code and data, but to make sure that it remains
accessible and to provide the means to test it even as technology
progresses and operating systems, file formats, and software libraries
change. That may be too much to ask right now; what seems reasonable,
though, is to ask of researchers in attack detection mechanisms not to
buck the trend and to take steps to increase the availability of their
research.

\subsection{Computational Overhead and Deployment Remarks}

Computational overhead comes up as a non-trivial issue. We observe
that the average overhead that can be extracted from the publications
that provide the overhead in percentage terms is 17.45\%. Relative
research on protection mechanisms~\cite{SPWS13} indicates that
mechanisms that introduce an overhead larger than 10\% do not tend to
gain wide adoption in production environments. Hence, the
computational overhead of some mechanisms could be a reason why they
have not been adopted.

The deployment difficulties we have found with many mechanisms are not
a reason not to adopt them, but they may hinder their widespread use.
Since source code injection attacks are complex, it may be logical to
expect that mechanisms to detect them would be complex too. The effort
to install and use a tool should be weighed against the expected
benefits. That is one more reason why it is important to report tool
accuracy, as this provides an immediate indicator to the expected
benefits. 

The issue is more acute when the point of detection is at the client.
We saw that many policy enforcement mechanisms that detect attacks at
Point {\sc ub} are not easy to deploy because modifications are needed
both on the server and the client side.
Conversely, it may be possible to deliver
the tools to the user unobtrusively; for example, we saw that
Blueprint enforces policies on the client side by embedding a library
in the server's response to the client. In general, when developing a
new countermeasure, researchers should consider where they are
planning to detect attacks, observe the corresponding deployment
challenges, and try to mitigate them.

\section{Conclusions}
\label{sec:conclusion}

Despite many approaches that have been developed,
application attacks based on source code injection
have been consistently present for the last 15 years,
and it appears that they will continue to be.
Attackers seem to find new ways to introduce
malicious source code to applications by using a
variety of languages and techniques~\cite{HNSHS12,DKH14}.
Meanwhile, during the last decade, there are
numerous mechanisms that have been developed to
detect one or more of such attacks. However,
even if some of them share characteristics
(for instance, {\sc html} sanitization and {\tt eval} handling)
with supported frameworks like Mozilla's Content Security
Policy~\cite{cspMoz} ({\sc csp}) and Google Caja~\cite{M06,caja}
the majority of them are not used in practice.

In order for a security tool to be used in practice it must provide
some value to the user. In particular, the value should outweigh the
cost of its use. The cost need not be directly monetary, but may be
incurred from the time required to use the tool, any inconvenience
caused, false alarms that it may raise. These costs are related to the
issues we have been investigating here: poor testing, high overhead,
unavailabilty of source, deployment difficulties, compromised security. 

Enhancing any of these would not just increase the value of a piece of
research as a practical tool, it would increase its research value as
well. Accurate detection reporting would help in appraising different
approaches. Performance measurements can point to blind, unpractical,
alleys and focus effort elsewhere. Availability of source code
enhances basic scientific tasks like checking and replication. Ease of
deployment brings ease of experimentation. Secure methods can form the
basis for developing methods with more extensive coverage.

We saw that there are mechanisms that have been extensively tested in
terms of accuracy ({\sc sqlc}heck, {\sc amnesia},
libAnomaly), solve specific problems in an efficient way (Blueprint
and the system by Stock et al.) or have a low computational overhead
({\sc dsi} and the system by Phung et al.). There are also cases where
researchers have made their code available ({\sc beep} and {\sc
  sd}river) which can be a push towards the development of better
mechanisms. This argument has been raised by another {\sc s}o{\sc k}
paper~\cite{SPWS13}. Furthermore, testing defenses in a production
setting could also propel their adoption.

We hope that the exploitation model, analysis, and observations that
emerged from our research can be a reference point for researchers who
wish to develop new, practical countermeasures against web application
attacks.

% First, we introduced an exploitation model
% to show how defense mechanisms can be seen
% from a common point of view. Our model can
% be a reference point for researchers who wish to develop
% new countermeasures. Then we categorized the various
% mechanisms and analyzed them based on specific criteria.
% We observed that the various mechanisms

% This is due to the following reasons:
% a) the mechanisms have not been extensively tested in terms
% of accuracy,
% b) are hard to deploy because they require many modifications
% either on the client-side, the server-side, or both,
% c) they add significant overhead, and
% d) can be bypassed by attackers that know how they work.
% The motivation behind this research is to systematize
% and analyze previously proposed protection mechanisms.
% In this way we can highlight the advantages
% and disadvantages of each mechanism, enlist them
% into different categories, and see how they compare to each other.

% \begin{itemize}
% \item We believe that our research may facilitate researchers
% or practitioners who would like to use one of the
% aforementioned mechanisms and researchers who would like
% to build new ones. Especially in the second case,
% researchers can utilize our exploitation model as a
% starting point to develop their approach.
% \item We believe that forming a
% standard way to test such mechanisms in terms
% of accuracy and computational performance
% would be one of the reasons of adopting them.
% \end{itemize}

\bibliographystyle{IEEEtran}
\bibliography{questioning}

\end{document} 

