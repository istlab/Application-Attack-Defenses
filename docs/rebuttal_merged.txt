We thank the reviewers for their insightful comments.

Overall, we did not consider mechanisms that
have not been presented in research papers. We mention Caja
due to its importance and popularity.  Also, although
some of the presented works use
static/dynamic analysis, we do not
consider the broader area of static/dynamic analysis for
vulnerability detection (another paper covers that area [14]),
but focus only on protections against attacks
that might exploit such vulnerabilities.

Rev1:

Sensitivity and specificity should be more widely applicable,
but not enough data is given in the
original publications, as mentioned in VII.A. The
usefulness of such metrics in computer security has been advocated
by other researchers as mentioned in the same section.

It is true that there are too many N/As, especially
Table II, but this actually highlights the issues regarding
code availability and accuracy testing.

We agree that "injection" should be part of the title,
and we will incorporate it.

We will include FlowFox and JSand (both have >20 citations),
the paper presenting IFC in WebKit's
JavaScript Bytecode (a recent paper)
and discuss ADsafe etc. in the same way as Google Caja.
AdSentry, Contego, and JSFlow 
do not reach our citation threshold.

We agree that using "security" as a dimension
is misleading---we use it in the sense of "bypassability", which is 
orthogonal to "accuracy". We will replace it with a more specific term.

We fully agree with the point about the minimalism of the security discussion,
and will try to enrich this part. For instance,
approaches that provide systematic
arguments on why their designs are secure, are not as
"bypassable" as heuristic/threshold-based approaches.

We can update our model to show where the language and web
framework-level protection mechanisms detect an attack
(the point would be at the application level within the server).

The comments about Firing Range, XCS attacks, scriptless attacks,
and attacks that involve plug-ins are valid and we will address them.

Rev2:

The lack of code availability for most systems 
makes performing original tests challenging.
This is in fact one of our findings, as we explain in VII.B.

We attempted to equally cover every mechanism and then
highlight the reasons why a mechanism may be widely
used or not. In our conclusion section, we particularly mention
that CSP is one of the few that 
has been published and is widely used.
It is true that if a programmer uses
secure coding practices like prepared statements, then
SQLi protections are not required.
However, best programming practices are not always followed,
and this motivates proposals like SQLrand,
which might be easier to retrofit in existing code bases.

We use the 1/1.1 notation to distinguish some shared states by the two main ways
attacks in our model can be initiated---we will explore a more clear representation.
The point regarding PHP injection attacks is valid.
We tried to capture the most known injection attacks,
and we will incorporate this attack with a few minor modifications to the model.
Our model covers attacks that utilize DSL languages in general,
and in our categorization there are more than one countermeasures that
protect from XPath injection attacks.

Rev3:

We agree with the comment on the "ease of use" dimension and we will
address it. Also, the comment about the EiQ is
correct: it is DBAL. We will make our SQLi example clearer.

We fully agree that sensitivity and specificity
could be expressed in terms of a conditional probability---we will
include this in the paper.

Our discussion in pages 2 and 13 respectively
highlights that we cannot carry out a set of measurements,
calculate sensitivity and specificity, and then using {True|False}
{Positives|Negatives} calculate PPV and NPV from equations
3 and 4 directly. One has to go instead through equations 5 and 6.

Taking into account all related comments, we will
highlight the advantages of PPV and NPV.
The gist is that, indeed, PPV and NPV answer the question
about trusting a detection result; it is not always
possible to get data for them (since we may not know the prevalence),
but it may be worth it to try and get it, or this could be a new research
direction. Of course, if we do have such data, it is a way to
operationalize the protection a system provides.

The comment on the skewed data is valid and we can
elaborate on it more. The validity of test data is in fact
important independently of sensitivity and specificity;
skewed test data will give a wrong picture anyway, even if nobody
bothers to calculate the metrics. We do not know in the
papers we studied whether test data were skewed. We do find,
however, that: more tests could have been
carried out (Table I and VII.A) and the means to conduct tests should be
made available to third parties (Table II and VII.B).

Rev4:

We do not intend to criticize the lack of testing
in etiological mechanisms. We attempt to 
explore why such mechanisms are not widely used, and suggest
that this is one reason.

Regarding the comment about demanding prevalence,
it may be unfair to ask of researchers to provide PPV and NPV values for
their mechanisms---in fact, nobody does. In the
p.13 we explore what this would require---we will elaborate more.

Finally, we will discuss more incentives
that the community could provide to improve code availability.
