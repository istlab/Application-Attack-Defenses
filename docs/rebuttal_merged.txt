We thank the reviewers for their insightful comments.

General remarks:

We didn't consider mechanisms that haven't been presented in research papers. We
mention Caja due to its importance.

Although some of the presented works use static/dynamic analysis, we don't
consider the area of vulnerability detection (it's been covered [14]), but focus
on protections against attacks that exploit such vulnerabilities.

We'll address the issues indicated by the reviewers, e.g., the title, the point
where language-protection mechanisms detect an attack, the "ease-of-use"
dimension, the SQLi example, the failure of capturing PHPi in our model and the
model's 1/1.1 notation.

Rev1:

Sensitivity and specificity should be more applicable, but not enough data is
available (see VII.A). Their use has been advocated before ("Analyzing Computer
Security: A Threat/Vulnerability/Countermeasure Approach"---will be cited).

The many N/As (especially in Table II) actually highlight the code availability
and accuracy testing issues.

We'll include FlowFox and JSand, the paper presenting IFC in WebKit's
JavaScript-Bytecode, and discuss ADsafe etc. AdSentry, Contego, and JSFlow don't
reach our citation threshold.

Using "security" as a dimension is indeed misleading---we mean
ease-of-circumvention, which is orthogonal to "accuracy". We'll replace it with
a more specific term.

We'll enrich the security discussion. For instance, approaches that provide
systematic arguments on their design's security cannot be circumvented like
heuristic/threshold-based approaches.

Rev2:

The lack of code availability makes performing original tests challenging, which
is one of our findings (VII.B).

We attempt to cover mechanisms and then highlight reasons why they may not be
widely used. In the conclusions we mention that CSP is one of the few that has
been published and is used.

When programmers use secure coding practices like prepared statements, then SQLi
protections aren't required. However, that may not happen, and this motivates
proposals like SQLrand, which might be easier to retrofit.

Our model covers attacks that utilize DSLs in general, and in our categorization
there are more than one countermeasures that protect from XPathi.

Rev3:

Sensitivity and specificity could be expressed in terms of a conditional
probability---we'll include this in the paper.

P.2 and p.13 highlight that we cannot perform a set of measurements, calculate
sensitivity and specificity, and then using {True|False} {Positives|Negatives},
calculate PPV and NPV from equations 3&4. One has to go instead through
equations 5&6.

We'll expand on the advantages of PPV and NPV. They answer the question about
trusting a detection result; however, it isn't always possible to get data for
them as we indicate in p.13.

The comment on the skewed data is valid. We don't know if data was skewed but we
find, that: more tests could have been performed (Table I and VII.A) and testbed
details should be available (Table II and VII.B).

Rev4:

We don't intend to criticize the lack of testing in etiological mechanisms. We
attempt to explore why such mechanisms aren't used, and suggest that this is one
reason. 

About demanding prevalence: it may be unfair to ask researchers to provide PPV
and NPV values. In p.13 we explore what this would require---we'll elaborate.
